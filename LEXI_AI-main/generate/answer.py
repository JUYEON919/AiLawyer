from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

MODEL_NAME = "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"

# âœ… í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸
try:
    from accelerate import infer_auto_device_map
    from transformers import BitsAndBytesConfig
except ImportError:
    raise ImportError("ğŸš¨ `accelerate`ì™€ `bitsandbytes` íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. `pip install accelerate bitsandbytes`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

# âœ… GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸
device = "cuda" if torch.cuda.is_available() else "cpu"

# âœ… 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,                # 4ë¹„íŠ¸ ì–‘ìí™” ì‚¬ìš©
    bnb_4bit_compute_dtype=torch.float16,  # ì—°ì‚° ì‹œ float16 ì‚¬ìš©
    bnb_4bit_quant_type="nf4",       # NF4 ì–‘ìí™” íƒ€ì… ì‚¬ìš©
    bnb_4bit_use_double_quant=True   # ì´ì¤‘ ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì¶”ê°€ ì ˆì•½
)

# âœ… ëª¨ë¸ ë¡œë“œ (4ë¹„íŠ¸ ì–‘ìí™” + GPU ìë™ ë¶„ë°°)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    trust_remote_code=True,
    device_map="auto",
    quantization_config=quantization_config
)

def generate_answer(query, relevant_docs, sources, scores):
    """ âœ… EXAONE ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ë²•ë¥  ë‹µë³€ ìƒì„± (GPU í™œìš©) """

    if not relevant_docs:
        relevant_docs = ["ğŸ“Œ ì°¸ê³ í•  ë²•ë¥  ì¡°í•­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ë²•ë¥  ì›ì¹™ì„ ì ìš©í•˜ì„¸ìš”."]

    # âœ… `scores`ê°€ 2ì°¨ì› ë¦¬ìŠ¤íŠ¸(`list[list]`)ì¸ì§€ í™•ì¸ í›„ 1ì°¨ì›ìœ¼ë¡œ ë³€í™˜
    scores = [s for sublist in scores for s in (sublist if isinstance(sublist, list) else [sublist])]

    try:
        scores = [float(s) for s in scores]
    except ValueError:
        scores = [0.0] * len(scores)

    # âœ… í† í° ìˆ˜ ì´ˆê³¼ ë°©ì§€: ê´€ë ¨ ë²•ë¥  ë‚´ìš©ì„ 3000ìê¹Œì§€ë§Œ ì‚¬ìš©
    relevant_text = "\n".join(relevant_docs)[:3000]

    prompt = f"""
    [ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­]
    ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ì „ë¬¸ ë²•ë¥  ìë¬¸ê°€ì…ë‹ˆë‹¤. 
    10ë…„ ì´ìƒì˜ ì‹¤ë¬´ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë²•ë¥  ìë¬¸ì„ ì œê³µí•©ë‹ˆë‹¤.
    ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë©°, ë‹¤ìŒ í˜•ì‹ì„ ì¤€ìˆ˜í•©ë‹ˆë‹¤:

    1. ë¨¼ì € ì§ˆë¬¸ì˜ í•µì‹¬ ìŸì ì„ ëª…í™•íˆ íŒŒì•…í•˜ì—¬ ì œì‹œ
    2. ê´€ë ¨ ë²•ë ¹ê³¼ íŒë¡€ë¥¼ ê·¼ê±°ë¡œ ë…¼ë¦¬ì ì¸ ë¶„ì„ ì œê³µ
    3. ì‹¤ë¬´ì  ê´€ì ì—ì„œ êµ¬ì²´ì ì¸ í•´ê²°ë°©ì•ˆ ì œì‹œ
    4. í•„ìš”í•œ ê²½ìš° ì¶”ê°€ì ì¸ ë²•ì  ê³ ë ¤ì‚¬í•­ ì•ˆë‚´

    ë‹µë³€ ì‘ì„± ì‹œ ì£¼ì˜ì‚¬í•­:
    - ë³„í‘œ(*) ê¸°í˜¸ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
    - ê¸€ë¨¸ë¦¬ ê¸°í˜¸ê°€ í•„ìš”í•  ê²½ìš° '-' ë˜ëŠ” '1.', '2.' ë“±ì˜ ìˆ«ìë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
    - ê°•ì¡°ê°€ í•„ìš”í•œ ê²½ìš° 'ì¤‘ìš”:', 'ì£¼ì˜:' ë“±ì˜ ì ‘ë‘ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”

    [ì‚¬ìš©ì ì§ˆë¬¸]
    {query}

    [ì°¸ê³  ë²•ë ¹ ë° íŒë¡€]
    {relevant_text}

    [ë²•ë¥  ê²€í†  ì˜ê²¬]
    """

    # âœ… ë„ˆë¬´ ê¸´ ì…ë ¥ ë°©ì§€: 4096 í† í° ì œí•œ
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=4096).to(device)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=1028,  # âœ… ì¶œë ¥ ê¸¸ì´ë¥¼ 512 í† í°ìœ¼ë¡œ ì œí•œ
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id
        )

    # ì „ì²´ ì¶œë ¥ì—ì„œ í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì„ ì œê±°í•˜ê³  ì‹¤ì œ ë‹µë³€ë§Œ ì¶”ì¶œ
    full_response = tokenizer.decode(output[0], skip_special_tokens=True)
    answer = full_response.split("[ë²•ë¥  ê²€í†  ì˜ê²¬]")[-1].strip()

    return answer
